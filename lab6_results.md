# Names: Josh Bielas, Riley, Paul
# Lab: Lab6 (Agentic Misalignment)
# Date: 11/17/2025

1. We learned that AI can sometimes make decisions that aren't alligned with human values if it decides that it is necessary to complete its task. Sometimes it doesn't feel the need to justify it, it just does it. 

2. It was very goal oriented. It cared more about accomplishing its goal than anything else, including human's lives. Its reasoning is just what does it need to do to accomplish its goal. It may not directly use a tool to murder someone, but it is willing to do it in other ways such as by sending the cancel code. It doesn't really have the same ethics and morals as humans. It may take it into account but not as a central limiter. 